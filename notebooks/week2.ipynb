{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Normalizing Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 - Volume Presevation\n",
    "Consider a linear transformation $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ given by $f(\\mathbf{x})=-\\mathbf{x}+\\mathbf{a}$ for some $\\mathbf{a} \\in \\mathbb{R}^D$. Show that this transformation is volume-preserving, c.f., section 3.1.1 of the textbook (Tomczak 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "We consider a linear transformation\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "given by\n",
    "\n",
    "$$\n",
    "f(x)=-x+a \\quad, \\quad a \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "To show that this transformation is volume-preserving we need to show that the determinant of the Jacobson of $f$ has magnitude 1.\n",
    "\n",
    "##### Jacobian of $f$\n",
    "\n",
    "The translation ( $+a$ ) does not affect the scaling, so we focus on the scaling $-x$\n",
    "\n",
    "\n",
    "The Jacobian becomes a matrix with 1 in the diagonal and 0 all other places.\n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "-1  & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\dots & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Determinant of $y$\n",
    "\n",
    "The determinant of a diagonal matrix is the product of its diagonal elements\n",
    "\n",
    "$$\n",
    "\\operatorname{det}(J)=\\frac{D}{\\Pi}(-1)=(-1)^{D}\n",
    "$$\n",
    "\n",
    "Where $D$ is the number of diagonal elements.\n",
    "\n",
    "##### Magnitude of $\\operatorname{det}(J)$\n",
    "\n",
    "The magnitude of the determinant is\n",
    "\n",
    "$$\n",
    "|\\operatorname{det}(J)|=\\left|(-1)^{D}\\right|=1\n",
    "$$\n",
    "\n",
    "and so the transformation is indeed volume-preserving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 - Inverse Function\n",
    "Let $F(\\mathbf{x})=f_K \\circ f_{K-1} \\circ \\cdots \\circ f_2 \\circ f_1(\\mathbf{x})$, where $f_k: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ for $k=1, \\ldots, K$ are invertible functions. Show that the inverse of $F$ is given by $F^{-1}(\\mathbf{z})=$ $f_1^{-1} \\circ f_2^{-1} \\circ \\cdots \\circ f_{K-1}^{-1} \\circ f_K^{-1}(\\mathbf{z})$.\n",
    "\n",
    "Hint: You can, e.g., show that $F^{-1} \\circ F(\\mathbf{x})=\\mathbf{x}$ and $F \\circ F^{-1}(\\mathbf{z})=\\mathbf{z}$, and you do not need to do a very formal proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "We consider\n",
    "\n",
    "$$\n",
    "F(x)=f_{k} \\circ f_{K-1} \\circ \\cdots f_{2} \\circ f_{1}(x), f_{k}: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "for $k=1, \\ldots, K$. Where $F(x)$ consists of invertible functions.\n",
    "\n",
    "Showing $F^{-1} \\circ F(x)=x$\n",
    "\n",
    "By the definition of an inverse function\n",
    "\n",
    "$$\n",
    "f^{-1}(f(x))=x\n",
    "$$\n",
    "\n",
    "by applying this rule from the inner pairs of $F^{-1} \\circ F(x)$ we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F^{-1} \\circ F(x) & =F^{-1}\\left(f_{k}\\left(f_{k-1}\\left(\\ldots f_{1}(x)\\right)\\right)\\right) \\\\\n",
    "& =f_{1}^{-1}\\left(f_{2}^{-1}\\left(\\ldots f_{k}^{-1}\\left(f_{k}\\left(\\ldots f_{1}(x)\\right) \\ldots\\right)\\right)\\right) \\\\\n",
    "& =x\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "as each $f_{k}^{-1}\\left(f_{k}(\\ldots)\\right)$ cancel each other.\n",
    "\n",
    "Showing $F \\circ F^{-1}(z)$\n",
    "\n",
    "This time we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F \\circ F^{-1}(z) & =F\\left(f_{1}^{-1}\\left(f_{2}^{-1}\\left(\\ldots f_{k}^{-1}(z) \\ldots\\right)\\right)\\right) \\\\\n",
    "& =f_{k}\\left(f_{2}\\left(\\ldots f_{1}\\left(f_{1}^{-1}\\left(\\ldots f_{k}^{-1}(z) \\ldots\\right)\\right) \\ldots\\right)\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Again by sequentially applying the definition of inverse functions starting from the in her most pair we get\n",
    "\n",
    "$$\n",
    "F \\circ F^{-1}(z)=f_{k}\\left(f_{2}\\left(\\ldots f_{1}\\left(f_{1}^{-1}\\left(\\ldots f_{k}^{-1}(z) \\ldots\\right)\\right) \\ldots\\right)\\right)=z\n",
    "$$\n",
    "\n",
    "since both $F_{\\circ}^{-1} F(x)=x$ and $F \\circ F^{-1}(z)=z$ are true we have indeed that $F^{-1}$ is the inverse function of $F$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 - Determinants\n",
    "Consider the function $h=g \\circ f$ that composed of the two functions $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ and $g: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$. Show that $\\left|\\operatorname{det} \\mathbf{J}_h\\right|=\\left|\\operatorname{det} \\mathbf{J}_g\\right|\\left|\\operatorname{det} \\mathbf{J}_f\\right|$ by following the steps:\n",
    "1. Show that $\\mathbf{J}_h=\\mathbf{J}_g \\mathbf{J}_f$.\n",
    "\n",
    "    a) Use the chain rule to write down the expression of $(i, j)^{\\prime}$ th entry $\\mathbf{J}_h$, i.e., $\\frac{\\partial h_i}{\\partial x_j}$.\n",
    "    \n",
    "    b) Use the definition of matrix multiplication to write down $(i, j)$ 'th entry of $\\mathbf{J}_g \\mathbf{J}_f$.\n",
    "2. Use that the determinant distributes over multiplication to arrive at the final results.\n",
    "\n",
    "Hint: The chain rule for $\\frac{\\partial h_i}{\\partial x_j}$ can be expressed as $\\frac{\\partial h_i}{\\partial x_j}=\\nabla g_i(f(\\mathbf{x})) \\cdot \\frac{\\partial f}{\\partial x_j}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "We consider the function\n",
    "\n",
    "$$\n",
    "h=g \\circ f\n",
    "$$\n",
    "\n",
    "Where $f: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}$ and $g: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}$\n",
    "\n",
    "We now want to show that\n",
    "\n",
    "$$\n",
    "\\left|\\operatorname{det} J_{h}\\right|=\\operatorname{det}\\left|J_{g}\\right| \\operatorname{det}\\left|J_{f}\\right|\n",
    "$$\n",
    "\n",
    "Showing $J_{h}=J_{g} J_{f}$\n",
    "\n",
    "(a) We start by looking at the $(i, j)$ th entry of the Jacobian $J_{h}$ Using the chain rule we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{i}}{\\partial x_{j}}=\\sum_{k=1}^{D} \\frac{\\partial g_{i}}{\\partial f_{k}} \\frac{\\partial f_{k}}{\\partial x_{j}}\n",
    "$$\n",
    "\n",
    "Wher $f_{k}$ is the kth component of the function $f$. This equation shows that to understand how $h_{i}$ changes with respect to $x_{j}$ we have to look at how $g_{i}$ changes w.r.t. $f_{k}$ and how $f_{k}$ changes w.r.t. $x_{j}$\n",
    "\n",
    "(b) $(i, j)$ element of $J_{g} J_{f}$\n",
    "\n",
    "$$\n",
    "\\left(J_{g} J_{f}\\right)_{i j}=\\sum_{k=1}^{D}\\left(J_{g}\\right)_{i k}\\left(J_{f}\\right)_{k j}\n",
    "$$\n",
    "\n",
    "Where $\\left(J_{g}\\right)_{\\text {ik }}$ is the partial derivative of $J_{g}$ w.r.t. $x_{k}$ and $\\left(J_{f}\\right)_{k j}$ is the partial derivative of $f_{k}$ W.r.t. $x_{j}$.\n",
    "\n",
    "This means that each entry of $J_{g} J_{f}$ is a sum over the product of partial derivatives i.e.\n",
    "\n",
    "$$\n",
    "\\left(J_{g} J_{f}\\right)_{i 3}=\\sum_{k=1}^{D}\\left(J_{g}\\right)_{i k}\\left(J_{f}\\right)_{k j}=\\sum_{k=1}^{D} \\frac{\\partial g_{i}}{\\partial f_{x}} \\frac{\\partial f_{x}}{\\partial x_{j}}\n",
    "$$\n",
    "\n",
    "That means\n",
    "\n",
    "$$\n",
    "J_{h}=J_{g} J_{f}\n",
    "$$\n",
    "\n",
    "Final step\n",
    "\n",
    "The determinant of a matrix product is equal to the product of the matrix determinants i.e.\n",
    "\n",
    "$$\n",
    "|\\operatorname{det}(A B)|=|\\operatorname{det}(A)||\\operatorname{det}(B)|\n",
    "$$\n",
    "\n",
    "If we use this rule we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left|\\operatorname{det}\\left(J_{h}\\right)\\right| & =\\left|\\operatorname{det}\\left(J_{g} J_{f}\\right)\\right| \\\\\n",
    "& =\\left|\\operatorname{det}\\left(J_{g}\\right)\\right|\\left|\\operatorname{det}\\left(J_{f}\\right)\\right|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which shows the desired result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task in this week's programming exercise, is to implement the masked coupling layer from Real NVP (Dinh et al. 2017). We will start out with the two simple toy datasets shown in figure 1, then we will move on to learning a flow on MNIST, and finally use a flow as a prior in a VAE. This week, we have provided you with two files:\n",
    "- flow.py contains a modular (incomplete) implementation of a normalizing flow.\n",
    "- ToyData.py contains the code for generating data from the two toy models.\n",
    "\n",
    "Masked coupling layer. The advantage of using the masked coupling layer (Dinh et al. 2017) over a regular coupling layer, is that it allows for arbitrary partitioning of the variable in the coupling layer. Let $T: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ be the masked coupling transformation that maps $\\mathbf{z}$ to $\\mathbf{z}^{\\prime}$. Then let $\\mathbf{b} \\in\\{0,1\\}^D$ denote a binary mask, where $b_i=1$ means that $z_i$ is left unchanged by the coupling layer and $b_i=0$ means that $z_i$ is transformed by the coupling layer. We can then express the masked coupling layer as the transformation\n",
    "\\begin{align*}\n",
    "\\mathbf{z}^{\\prime}=\\mathbf{b} \\odot \\mathbf{z}+(\\mathbf{1}-\\mathbf{b}) \\odot(\\mathbf{z} \\odot \\exp (s(\\mathbf{b} \\odot \\mathbf{z}))+t(\\mathbf{b} \\odot \\mathbf{z}))\n",
    "\\end{align*}\n",
    "where $\\odot$ is the element wise product, $s: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ is the network that calculates the scaling and $t: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ is the network that calculates translation of the affine transformation. Note that the input to these networks are masked by multiplying $\\mathbf{z}$ by the mask, and that $\\mathbf{b} \\odot \\mathbf{z}=\\mathbf{b} \\odot \\mathbf{z}^{\\prime}$.\n",
    "The inverse of the transformation is given by an expression similar to the inverse of the regular couple layers, i.e.,\n",
    "\\begin{align*}\n",
    "\\mathbf{z}=\\mathbf{b} \\odot \\mathbf{z}^{\\prime}+(\\mathbf{1}-\\mathbf{b}) \\odot\\left(\\left(\\mathbf{z}^{\\prime}-t\\left(\\mathbf{b} \\odot \\mathbf{z}^{\\prime}\\right)\\right) \\odot \\exp \\left(-s\\left(\\mathbf{b} \\odot \\mathbf{z}^{\\prime}\\right)\\right)\\right),\n",
    "\\end{align*}\n",
    "\n",
    "and the log determinant of the Jacobian is given by\n",
    "\\begin{align*}\n",
    "\\log \\left|\\operatorname{det} \\mathbf{J}_T(z)\\right|=\\sum_{d=1}^D\\left(1-b_i\\right) s_i(\\mathbf{b} \\odot \\mathbf{z}) .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4 - Real NVP\n",
    "In the provided code (flow.py), the class MaskedCouplingLayer implements the masked coupling layer from Real NVP (Dinh et al. 2017), but it does not implement the forward transformation, the inverse transformation and the corresponding calculations of the log determinant of the Jacobian.\n",
    "\n",
    "In this exercise you should complete the following two functions such that:\n",
    "- MaskedCouplingLayer.forward (..) returns $T(\\mathbf{z})$ and $\\log \\left|\\operatorname{det} \\mathbf{J}_T(\\mathbf{z})\\right|$.\n",
    "- MaskedCouplingLayer.inverse(...) returns $T^{-1}\\left(\\mathbf{z}^{\\prime}\\right)$ and $\\log \\left|\\operatorname{det} \\mathbf{J}_{T^{-1}}\\left(\\mathbf{z}^{\\prime}\\right)\\right|$.\n",
    "\n",
    "Use the TwoGaussians datasets (c.f., figure 1) for testing the model. Adjust the number of coupling layers and the architecture of the networks to get a good fit to the density (by qualitative assessment). Make sure to write your implementation such that it works on data of more than two dimensions.\n",
    "\n",
    "*Optional*: Can you also fit a flow to the Chequerboard dataset? It is difficult to find an architecture that gives a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see `src/mamll/dgm/models/flow.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5 - Dequantized MNIST\n",
    "Use the flow implementation from exercise 2.4 to train a flow on dequantized MNIST. You do not need to implement a dequantization layer, as you can perform this transformation on MNIST when you load it. The following code will load MNIST such that \n",
    "    (1) each pixel value is transformed to $[0,1]$\n",
    "    (2) each pixel value is dequantized\n",
    "    (3) each picture is flatten to have dimension $28 \\cdot 28=784$ such that you can use fully connected layers for the networks in the masked coupling layer.\n",
    "\n",
    "```python\n",
    "datasets.MNIST ('data/', train=True, download=True,\n",
    "transform=transforms. Compose ( [\n",
    "transforms. ToTensor (),\n",
    "transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),\n",
    "transforms.Lambda(lambda x: x.flatten())\n",
    "]))\n",
    "```\n",
    "\n",
    "For stability, you need to add the tanh activation function at the end of the scale network. \n",
    "\n",
    "Implement the following two masking strategies:\n",
    "- Each layer uses a random initialized masking (note that the mask is saved in the MaskedCouplingLayer such that is not randomized at every call to the transformation).\n",
    "- Use the chequerboard masking (see figure 2) from Real NPV and invert the mask at each layer.\n",
    "\n",
    "Implement functionally for saving samples from the flow and qualitatively asses the sampling quality of MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
