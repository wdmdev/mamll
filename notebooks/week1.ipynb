{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Deep latent variable models\n",
    "In particular the exercises concern VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Exercise 1.1 Consider probabilistic principal component analysis (PPCA), as described in section 4.2 of the textbook (Tomczak 2022). The PPCA model has three parameters $\\sigma^2, \\mathbf{b}$ and $\\mathbf{W}$. Given a data set $\\mathcal{D}=\\left\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\right\\}$, show that the maximum likelihood estimate of $\\mathbf{b}$ is the mean of the data set, i.e., $\\hat{\\mathbf{b}}=\\overline{\\mathbf{x}}$. Do this by following the steps:\n",
    "1. Based on equation (4.6), write the $\\log$-likelihood function, $\\ell\\left(\\sigma^2, \\mathbf{b}, \\mathbf{W}\\right)=\\ln p(\\mathcal{D} \\mid$ $\\left.\\sigma^2, \\mathbf{b}, \\mathbf{W}\\right)=\\sum_{n=1}^N \\ln p\\left(\\mathbf{x}_n \\mid \\sigma^2, \\mathbf{b}, \\mathbf{W}\\right)$.\n",
    "2. Set the derivative of the log-likelihood with respect to $\\mathbf{b}$ equal to zero.\n",
    "\n",
    "Hint: For a symmetric matrix $\\mathbf{W}$ and vectors $\\mathbf{x}$ and $\\mathbf{b}$, we have\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{b}}(\\mathbf{x}-\\mathbf{b})^{\\top} \\mathbf{W}(\\mathbf{x}-\\mathbf{b})=-2 \\mathbf{W}(\\mathbf{x}-\\mathbf{b}),\n",
    "$$\n",
    "see, e.g., equation (86) by Petersen and Pedersen (2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "\\ell\\left(\\sigma^{2}, b, w\\right)=\\sum_{n=1}^{N} \\ln p\\left(x_{n} \\mid \\sigma^{2}, b, w\\right)\n",
    "$$\n",
    "\n",
    "We define\n",
    "\n",
    "$$\n",
    "S=W W^{\\top}+6^{2} I\n",
    "$$\n",
    "\n",
    "We use 4.6 to define\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\ell\\left(\\sigma^{2}, b, W\\right)=\\sum_{n=1}^{N} \\ln p\\left(x_{n}\\right) \\\\\n",
    "& =\\sum_{n=1}^{N} \\ln N\\left(x_{n} \\mid b, S\\right) \\\\\n",
    "& =\\sum_{n=1}^{N} \\ln \\left[\\frac{1}{\\sqrt{(2 \\pi)^{d}|S|}} \\exp \\left(-\\frac{1}{2}\\left(x_{n}-b\\right)^{\\top} S^{-1}\\left(x_{n}-b\\right)\\right]\\right. \\\\\n",
    "& =\\sum_{n=1}^{N}\\left[-\\frac{d}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln |S|-\\frac{1}{2}\\left(x_{n}-b\\right)^{\\top} S^{-1}\\left(x_{n}-b\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we look at\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial b} \\ell\\left(\\sigma^{2} b, W\\right) & =\\sum_{n=1}^{N} \\frac{\\partial}{\\partial b}-\\frac{1}{2}\\left(x_{n}-b\\right)^{T} S^{-1}\\left(x_{n}-b\\right) \\\\\n",
    "& =-\\frac{1}{2} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial b}\\left(x_{n}-b\\right)^{T} S^{-1}\\left(x_{n}-b\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "we use the hint\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& -\\frac{1}{2} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial b}\\left(x_{n}-b\\right)^{T} S^{-1}\\left(x_{n}-b\\right)=\\sum_{n=1}^{N} S^{-1}(x-b) \\\\\n",
    "& =S^{-1} \\sum_{n=1}^{N}(x-b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We set the derivative equal to 0\n",
    "\n",
    "$$\n",
    "S^{-1} \\sum_{n=1}^{N}(x-b)=0\n",
    "$$\n",
    "\n",
    "and $S^{-1}$ is positive definite, so\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{n=1}^{N}(x-b)=0 \\Leftrightarrow \\\\\n",
    "& \\sum_{n=1}^{N} b=\\sum_{n=1}^{N} x \\Leftrightarrow \\\\\n",
    "& b=\\frac{1}{N} \\sum_{n=1}^{N} x\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Exercise 1.2 As discussed in section 4.3.1 of the textbook (Tomczak 2022), the second term of the ELBO in equation (4.17), can be written as a KL-divergence, i.e.,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{ELBO}(\\mathbf{x}) & =\\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}[\\ln p(\\mathbf{x} \\mid \\mathbf{z})]-\\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\ln q_\\phi(\\mathbf{z} \\mid \\mathbf{x})-\\ln p(\\mathbf{z})\\right] \\\\\n",
    "& =\\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}[\\ln p(\\mathbf{x} \\mid \\mathbf{z})]-\\operatorname{KL}\\left[q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the KL-divergence of density $f(\\mathbf{x})$ from density $g(\\mathbf{x})$ is defined as\n",
    "$$\n",
    "\\mathrm{KL}[f \\| g]=\\int f(\\mathbf{x}) \\ln \\frac{f(\\mathbf{x})}{g(\\mathbf{x})} \\mathrm{d} \\mathbf{x}\n",
    "$$\n",
    "When training VAEs, we often use that this KL-divergence has a closed for expression for many distributions, e.g., when both the approximate posterior $q_\\phi(\\mathbf{z} \\mid \\mathbf{x})$ and the prior $p(\\mathbf{z})$ are multivariate Gaussian.\n",
    "1. Show that the two expressions for the ELBO in equations (2) and (3) are equivalent, i.e., that $\\mathbb{E}_{z \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\ln q_\\phi(\\mathbf{z} \\mid \\mathbf{x})-\\ln p(\\mathbf{z})\\right]=\\operatorname{KL}\\left[q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right]$.\n",
    "2. Show that the KL divergence between two univariate Gaussian distributions has the following closed form\n",
    "$$\n",
    "\\mathrm{KL}\\left[\\mathcal{N}\\left(\\mu_1, \\sigma_1\\right) \\| \\mathcal{N}\\left(\\mu_2, \\sigma_2\\right)\\right]=\\ln \\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2+\\left(\\mu_1-\\mu_2\\right)^2}{2 \\sigma_2^2}-\\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "Exercise 1.3 Consider the two-level hierarchical VAE, as described in section 4.5 of the textbook (Tomczak 2022). Starting from the standard ELBO, e.g., equation (4.17), show that the ELBO for the two-level VAE can be written as equation (4.82), i.e.,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{ELBO}(\\mathbf{x})=\\mathbb{E}_{\\left(\\mathbf{z}_1, \\mathbf{z}_2\\right) \\sim Q\\left(\\mathbf{z}_1, \\mathbf{z}_2\\right)} & {\\left[\\ln p\\left(\\mathbf{x} \\mid \\mathbf{z}_1\\right)\\right.} \\\\\n",
    "& \\left.-\\mathrm{KL}\\left[q\\left(\\mathbf{z}_1 \\mid \\mathbf{x}\\right) \\| p\\left(\\mathbf{z}_1 \\mid \\mathbf{z}_2\\right)\\right]-\\operatorname{KL}\\left[q\\left(\\mathbf{z}_2 \\mid \\mathbf{z}_1\\right) \\| p\\left(\\mathbf{z}_2\\right)\\right]\\right] .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's programming exercise, you will work with variational autoencoders (VAEs). We are going to consider a binarised version of the MNIST dataset, where pixels with values over 0.5 are set to 1 and pixels with values less than 0.5 are set to 0 .\n",
    "\n",
    "The provided file vae_bernoulli.py contains a modular and simple implementation of a VAE with\n",
    "- a Gaussian prior, $p(\\mathbf{z})$,\n",
    "- a product of Bernoulli likelihood, $p(\\mathbf{x} \\mid \\mathbf{z})$,\n",
    "- a fully connected encoder and decoder network.\n",
    "\n",
    "The implementation makes use of torch.distributions for the various distributions, which substantially simplifies the code.\n",
    "\n",
    "It also implements a command line parser, so you can train a VAE on the CPU with a latent dimension of $M=10$ for 5 epochs and with a batch size of 128 and save the final model to the file model.pt using the command\n",
    "```sh\n",
    "python3 vae_bernoulli.py train --device cpu --latent-dim 10 \\\n",
    "--epochs 5 --batch-size 128 --model model.pt\n",
    "```\n",
    "\n",
    "After the model has been trained, you can sample from the trained model (saved in model.pt) and save the samples in samples.png, using\n",
    "\n",
    "```sh\n",
    "python3 vae_bernoulli.py sample --device cpu --latent-dim 10 \\\n",
    "--model model.pt --samples samples.png\n",
    "```\n",
    "\n",
    "In the following exercises, you will modify this VAE implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "Exercise 1.4 In this first exercise, you should just inspect the code in `vae_bernoulli.py`. Answer the following questions:\n",
    "\n",
    "#### Answer\n",
    "- How is the reparametrisation trick handled in the code?\n",
    "    * By using `rsample()`\n",
    "- Consider the implementation of the ELBO. What is the dimension of `self.decoder( z).log_prob(x)` and of `td.kl_divergence(q, self.prior.distribuion)`?\n",
    "    * It is equal to the batch size `M`\n",
    "- The implementation of the prior, encoder and decoder classes all make use of `td.Independent`. What does this do?\n",
    "    * It makes the given number of dimensions into independent events i.e. we use it with 1 as the final input which means that the\n",
    "    first dimension (the batch dimension) becomes independent, therefore we get `M` independent Gaussian ditributions.\n",
    "- What is the purpose using the function `torch.chunk` in `GaussianEncoder.forward`?\n",
    "    * It splits the output tensor of the encoder into two equal parts along the last dimension. The encoder output is `2*M`, so we get\n",
    "    `M` means and standard deviations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "Exercise 1.5 Add the following functionally to the implementation (`vae_bernoulli.py`) of the VAE with Bernoulli output distributions:\n",
    "- Evaluate the ELBO on the binarised MNIST test set.\n",
    "- Plot samples from the approximate posterior and colour them by their correct class label. Implement it such that you, for latent dimensions larger than two, $M>2$, do PCA and project the sample onto the first two principal components (e.g., using scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6\n",
    "Exercise 1.6 Extend the VAE with Bernoulli output distributions (`vae_bernoulli.py`) to use a mixture of Gaussian prior (MoG). We recommend using the `MixtureSameFamily` class from `torch.distributions`, but you are also welcome to implement it from scratch. For your implementation of the VAE with the MoG prior:\n",
    "- Evaluate the test set ELBO. Do you see better performance?\n",
    "- Plot the samples from the approximate posterior. How does it differ from the model with the Gaussian prior? Do you see better clustering?\n",
    "\n",
    "Remark: You will need to change the formulation of the ELOB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7\n",
    "Exercise 1.7 So far, we have considered a binarised version of MNIST. We will now consider the pixel values in MNIST to be continuous and experiment with different output distributions. You can load the continuous MNIST training set using\n",
    "```python\n",
    "datasets.MNIST ('data/', train=True, download=True,\n",
    "transform=transforms. Compose ( [\n",
    "transforms. ToTensor (),\n",
    "transforms. Lambda (lambda x: x.squeeze())\n",
    "]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a multivariate Gaussian output distribution. You should both experiment with learning the variance of each pixel and having a fixed variance for all pixels.\n",
    "- How is the qualitative sample quality?\n",
    "- Rathen then sampling from $p(\\mathbf{x} \\mid \\mathbf{z})$, try to sample $\\mathbf{z} \\sim p(\\mathbf{z})$ and then show the mean of the output distribution, $p(\\mathbf{x} \\mid \\mathbf{z})$. Does the mean qualitatively look better?\n",
    "\n",
    "Optional: You can also try the Continuous Bernoulli output distribution, see LoaizaGanem and Cunningham (2019) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.8\n",
    "Exercise 1.8 (Optional) Extend the VAE with Bernoulli output distributions (`vae_bernoulli .py`) to use a CNN based encoder and decoder. For your new implementation:\n",
    "- When sampling from a trained model, do you see a qualitative improvement in sample quality?\n",
    "- Evaluate the test set ELBO. Do you see better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
